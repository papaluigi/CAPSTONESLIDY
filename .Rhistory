)) +
theme(axis.title.y = element_text(size = 12)) +
theme(axis.title.x = element_text(size = 12)) +
theme(plot.title = element_text(size = 10)) +
theme(legend.text = element_text(size = 8)) +
theme(legend.title = element_text(size = 10)) +
theme(plot.title = element_text(size=12)) +
scale_y_continuous(labels = comma) +
ggtitle("Forecast Revenue per month")
print(ptype)
}
#plotgenFctRevDyn(data[data$Month >= "2014-03-01",],"cluster")
#plotgenFctRevDyn(data[data$Month >= "2014-03-01",],"cluster","FCTREV")
#ISSUE HERE
plotgenFctRevDyn(data[data$Month >= "2014-03-01",],"cluster","FCTREV")
data[data$Month >= "2014-03-01",]
plotgenFctRevDyn(data[data$Month >= "2014-03-01",],"cluster","FCTREV")
gfctsnap <- ggplot(aggrFCTREV[aggrFCTREV$Month==endperiod,], aes(MU, x, fill=MU))
pfctsnap <- gfctsnap +
geom_bar(stat="identity") +
#geom_boxplot() +
#geom_jitter() +
#theme(legend.position="none", panel.grid.minor = element_line(size=0.5)) +
theme(legend.position="none", axis.text.x = element_text(size=10, angle=45, hjust=1, vjust=1)) +
#geom_text(aes(label=formatC(x, format="e",digits=1, big.mark=".")), vjust=0, size=4) +
geom_text(aes(label=paste(formatC(x/1000000, format="f",digits=1, big.mark="."),"mEUR")), vjust=-1, size=4) +
ylab("Forecast Revenue (EUR)") +
scale_y_continuous(labels = comma, minor_breaks=seq(0, 110000000, 5000000)) +
ggtitle("Forecast Revenue per MU")
print(pfctsnap)
View(aggrFCTREV)
aggrFCTREV <- aggregate(data$FCTREV, list(Month = data$Month, MU=data$MU), sum, na.rm=TRUE)
gfctsnap <- ggplot(aggrFCTREV[aggrFCTREV$Month==endperiod,], aes(MU, x, fill=MU))
pfctsnap <- gfctsnap +
geom_bar(stat="identity") +
#geom_boxplot() +
#geom_jitter() +
#theme(legend.position="none", panel.grid.minor = element_line(size=0.5)) +
theme(legend.position="none", axis.text.x = element_text(size=10, angle=45, hjust=1, vjust=1)) +
#geom_text(aes(label=formatC(x, format="e",digits=1, big.mark=".")), vjust=0, size=4) +
geom_text(aes(label=paste(formatC(x/1000000, format="f",digits=1, big.mark="."),"mEUR")), vjust=-1, size=4) +
ylab("Forecast Revenue (EUR)") +
scale_y_continuous(labels = comma, minor_breaks=seq(0, 110000000, 5000000)) +
ggtitle("Forecast Revenue per MU")
print(pfctsnap)
shiny::runApp('Portfolio')
shiny::runApp('Portfolio')
shiny::runApp('Portfolio')
getwd()
shiny::runApp('Portfolio')
getwd()
source(file="Portfolio/Portfolio_foundation.R")
shiny::runApp('Portfolio')
uterm1 <- unique(Nfreq2W$X1)
load("~/R/LFG Working Directory/CAPSTONE/CAP01.RData")
setwd("~/R/LFG Working Directory/CAPSTONE")
View(freq1W)
set.seed(4321)
blog_data <- file(paste(datafolder, fileList, sep="/")[1], open="r")
blog_lines <- readLines(blog_data, encoding="UTF-8")
num_blog_lines <- length(blog_lines)
blog_sample <- blog_lines[sample(1:num_blog_lines, num_blog_lines * 0.1, replace=FALSE)]
close(blog_data)
# Let's do some space
rm(blog_lines)
set.seed(4321)
blog_data <- file(paste(datafolder, fileList, sep="/")[1], open="r")
blog_lines <- readLines(blog_data, encoding="UTF-8")
options(java.parameters = "-Xmx1024m")
library(NLP)
library(tm)
library(RWeka)
library(SnowballC)
# Let's clean the sample from the potential UTF-8 issues !
clean_blog_sample <- sapply(blog_sample, function(x) iconv(enc2utf8(x), "UTF-8", "ASCII", sub = " "))
set.seed(4321)
num_blog_lines <- length(clean_blog_sample)
train_ind <- sample(1:num_blog_lines, num_blog_lines * 0.5, replace=FALSE)
clean_blog_train <- clean_blog_sample[train_ind]
clean_blog_test <- clean_blog_sample[-train_ind]
#clean_blog_sample <- stri_encode(blog_sample, "", "UTF-8")
# Let's create a single document corpus
blog_Corp <- Corpus(VectorSource(list(clean_blog_train))) #CREATES A SINGLE DOC
#blog_Corp <- Corpus(DataframeSource(clean_blog_sample))
#blog_Corp <- Corpus(VectorSource(clean_blog_sample)) #CREATES MULTIPLE DOCS
# Let's clean the memory
rm(clean_blog_sample)
rm(clean_blog_train)
# Let's apply some transformations
#blog_Corp <- tm_map(blog_Corp, content_transformer(removePunctuation)) #NOT WORKING
f_rempunc <- content_transformer(function(x, pattern) sub(pattern, " ", x))
blog_Corp <- tm_map(blog_Corp, f_rempunc, "[[:punct:]]")
blog_Corp <- tm_map(blog_Corp, content_transformer(tolower))
blog_Corp <- tm_map(blog_Corp, removeWords, c("fuck","boobs")) # Just for test
blog_Corp <- tm_map(blog_Corp, stripWhitespace)
#blog_Corp <- tm_map(blog_Corp, stemDocument, language="english")
#blog_Corp <- tm_map(blog_Corp, stemCompletion, dictionary=dictCorpus)
clean_blog_sample <- sapply(blog_sample, function(x) iconv(enc2utf8(x), "UTF-8", "ASCII", sub = " "))
set.seed(4321)
num_blog_lines <- length(clean_blog_sample)
train_ind <- sample(1:num_blog_lines, num_blog_lines * 0.5, replace=FALSE)
clean_blog_train <- clean_blog_sample[train_ind]
clean_blog_test <- clean_blog_sample[-train_ind]
#clean_blog_sample <- stri_encode(blog_sample, "", "UTF-8")
# Let's create a single document corpus
blog_Corp <- Corpus(VectorSource(list(clean_blog_train))) #CREATES A SINGLE DOC
#blog_Corp <- Corpus(DataframeSource(clean_blog_sample))
#blog_Corp <- Corpus(VectorSource(clean_blog_sample)) #CREATES MULTIPLE DOCS
# Let's clean the memory
rm(clean_blog_sample)
rm(clean_blog_train)
# Let's apply some transformations
#blog_Corp <- tm_map(blog_Corp, content_transformer(removePunctuation)) #NOT WORKING
f_rempunc <- content_transformer(function(x, pattern) sub(pattern, " ", x))
blog_Corp <- tm_map(blog_Corp, f_rempunc, "[[:punct:]]")
blog_Corp <- tm_map(blog_Corp, content_transformer(tolower))
blog_Corp <- tm_map(blog_Corp, removeWords, c("fuck","boobs")) # Just for test
blog_Corp <- tm_map(blog_Corp, stripWhitespace)
# Now let's create the N-Gram Tokenizer functions for single words, Bi-Grams and Tri-Grams
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm <- TermDocumentMatrix(blog_Corp, control=list(tokenize=UnigramTokenizer))
freq1W <- data.frame(as.matrix(tdm))
freq1W$term <- rownames(freq1W)
freq1W <- freq1W[order(-freq1W$X1),]
freq1W$index <- 1:nrow(freq1W)
colnames(freq1W) <- c("N_Uni","term","index")
topfreq <- head(freq1W, n=20)
par(las=2) # X names vertical
barplot(height=topfreq$X1, names.arg = topfreq$term, cex.names=1, main="Top 20 unique words used in the corpus")
freq1W <- data.frame(as.matrix(tdm))
freq1W$term <- rownames(freq1W)
freq1W <- freq1W[order(-freq1W$X1),]
freq1W$index <- 1:nrow(freq1W)
colnames(freq1W) <- c("N_Uni","term","index")
topfreq <- head(freq1W, n=20)
par(las=2) # X names vertical
barplot(height=topfreq$X1, names.arg = topfreq$term, cex.names=1, main="Top 20 unique words used in the corpus")
View(topfreq)
barplot(height=topfreq$N_uni, names.arg = topfreq$term, cex.names=1, main="Top 20 unique words used in the corpus")
barplot(height=topfreq$N_Uni, names.arg = topfreq$term, cex.names=1, main="Top 20 unique words used in the corpus")
cs <- data.frame(cbind(1:length(freq1W$term), cumsum(freq1W$N_Uni)))
names(cs) <- c("Num","SumInst")
plot(cs, main="Cumulated Frequencies of unique words", sub="From top freq. to lowest freq.", xlab="Unique words Index", ylab="Cumulated freq.")
# Calculate 50% threshold and add on plot in red
th50 <- tail(cs[cs$SumInst<=tail(cumsum(freq1W$N_Uni)*0.5,n=1),], n=1)
abline(v=th50$Num, col="red")
abline(h=th50$SumInst, col="red")
mtext(th50$Num, side=1, line=2, at=th50$Num, col="red")
mtext("50%", side=2, line=2, at=th50$SumInst, col="red")
# Calculate 90% threshold and add on plot in blue
th90 <- tail(cs[cs$SumInst<=tail(cumsum(freq1W$N_Uni)*0.9,n=1),], n=1)
abline(v=th90$Num, col="blue")
abline(h=th90$SumInst, col="blue")
mtext(th90$Num, side=1, line=2, at=th90$Num, col="blue")
mtext("90%", side=2, line=2, at=th90$SumInst, col="blue")
length(freq1W$N_Uni)
library(plyr)
library(dplyr)
# GT matrix creation for unigrams
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
View(GT1)
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
View(GT1)
View(GT1)
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1$Ntot <- GT1$N * GT1*c
GT1$Ntot <- GT1$N * GT1$c
sum(GT$Ntot)
sum(GT1$Ntot)
GT1$Ntot_star <- GT1$N * GT1$c_star
sum(GT1$Ntot_start)
sum(GT1$Ntot_start)
sum(GT1$Ntot_star)
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
View(GT1)
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
View(GT1)
GT1 <- head(GT1, n=10)
GT1$Ntot <- GT1$c * GT1$N
GT1$Ntot_star <- GT1$c_star * GT1$N
sum(GT1$Ntot-GT1$Ntot_star)
nTerms(tdm)
# GT matrix creation for unigrams
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1 <- head(GT1, n=7)
GT1$NTot <- GT1$c * GT1$N
GT1$NTot_star <- GT1$c_star * GT1$N
sum(GT1$NTot) - sum(GT1$NTot_star)
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1 <- head(GT1, n=20)
GT1$NTot <- GT1$c * GT1$N
GT1$NTot_star <- GT1$c_star * GT1$N
diff <- sum(GT1$NTot) - sum(GT1$NTot_star)
print(diff/GT1$N[1])
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1 <- head(GT1, n=50)
GT1$NTot <- GT1$c * GT1$N
GT1$NTot_star <- GT1$c_star * GT1$N
diff <- sum(GT1$NTot) - sum(GT1$NTot_star)
print(diff/GT1$N[1])
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1 <- head(GT1, n=100)
GT1$NTot <- GT1$c * GT1$N
GT1$NTot_star <- GT1$c_star * GT1$N
diff <- sum(GT1$NTot) - sum(GT1$NTot_star)
print(diff/GT1$N[1])
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1 <- head(GT1, n=5)
GT1$NTot <- GT1$c * GT1$N
GT1$NTot_star <- GT1$c_star * GT1$N
diff <- sum(GT1$NTot) - sum(GT1$NTot_star)
print(diff/GT1$N[1])
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1 <- head(GT1, n=200)
GT1$NTot <- GT1$c * GT1$N
GT1$NTot_star <- GT1$c_star * GT1$N
diff <- sum(GT1$NTot) - sum(GT1$NTot_star)
print(diff/GT1$N[1])
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1 <- head(GT1, n=5)
GT1$NTot <- GT1$c * GT1$N
GT1$NTot_star <- GT1$c_star * GT1$N
diff <- sum(GT1$NTot) - sum(GT1$NTot_star)
print(diff/GT1$N[1])
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
Ntoken <- sum(GT1$c * GT1$N)
# GT matrix creation for unigrams
GT1 <- ddply(freq1W, .(N_Uni), summarise, N=length(N_Uni))
colnames(GT1) <- c("c","N")
Ntoken <- sum(GT1$c * GT1$N)
for (i in 1:length(GT1$c)){
GT1$c_star[i] <- (GT1$c[i]+1)*GT1$N[GT1$c[i]+1]/GT1$N[GT1$c[i]]
}
GT1 <- head(GT1, n=5)
GT1$NTot <- GT1$c * GT1$N
GT1$NTot_star <- GT1$c_star * GT1$N
GT1$P_star <- GT1$c_star / Ntoken
diff <- sum(GT1$NTot) - sum(GT1$NTot_star)
print(diff/GT1$N[1])
Ntoken
35443/Ntoken
diff
diff/GT1$N[1]/Ntoken
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
nextw <- function(word_seq, res){
ws_vec <- strsplit(as.character(word_seq), split=" ")
l <- length(ws_vec[[1]]) # Number of words in the sequence
if(substr(word_seq,(nchar(word_seq)+1)-1,nchar(word_seq)) == " ") l <- l+1
if (l>=3){
# Search in trigram table based upon words -3, -2 and -1
#ind <- grep(paste("^", word(word_seq, -3), "$", sep=""), Nfreq3W$X1)
ind <- which(Nfreq3W$X1==word(word_seq, -3))
df_tri <- Nfreq3W[ind,]
#ind <- grep(paste("^", word(word_seq, -2), "$", sep=""), df_tri$X2)
ind <- which(df_tri$X2==word(word_seq, -2))
df_tri <- df_tri[ind,]
ind <- grep(paste("^", word(word_seq, -1), sep=""), df_tri$X3)
res <- df_tri[ind,]
if (length(res$X3)==0) l <- 2
}
if (l==2){
# Search in bigram table based upon words -2 and -1
#ind <- grep(paste("^", word(word_seq, -2), "$", sep=""), Nfreq2W$X1)
ind <- which(Nfreq2W$X1==word(word_seq, -2))
df_bi <- Nfreq2W[ind,]
ind <- grep(paste("^", word(word_seq, -1), sep=""), df_bi$X2)
res <- df_bi[ind,]
if (length(res$X2)==0) l <- 1
}
if (l==1) {
# Search in unigram table based upon word being typed
ind <- grep(paste("^", word(word_seq, -1), sep=""), freq1W$term)
res <- freq1W[ind,]
}
res <- head(res,3)
print(l)
print(res[,ncol(res)])
#return(res)
rownames(res) <- res[,ncol(res)]
return(res[,ncol(res)])
}
nextw("hel", res)
res
nextw("hel", )
nextw("hel", )[1]
nextw("hel", )[,1]
nextw("hel", )
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
nchar("hello") >2
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
rm(freq1W)
freq1W <- read.csv2(file="freq1W_th90.csv")
View(freq1W)
View(freq2W)
View(Nfreq2W)
shiny::runApp('SHAPP')
View(Nfreq2W)
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
View(freq1W)
View(Nfreq3W)
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
shiny::runApp('SHAPP')
View(Nfreq2W)
shiny::runApp('SHAPP')
ind <- which(Nfreq2W$X1==word("i ", -2))
ind
Nfreq2W[ind,]
head(Nfreq2W[ind,])
ind <- grep(paste("^", word("i  w", -1), sep=""), df_bi$term)
ind <- grep(paste("^", word("i  w", -1), sep=""), Nfreq2W$term)
res <- Nfreq2W[ind,]
View(res)
shiny::runApp('SHAPP')
Nfreq3W <- read.csv2(file="Nfreq3W.csv")
colnames(Nfreq3W)[which(names(Nfreq3W) == "freq3W.X1")] <- "Freq"
colnames(Nfreq3W)[which(names(Nfreq3W) == "X3")] <- "term"
Nfreq2W <- read.csv2(file="Nfreq2W.csv")
colnames(Nfreq2W)[which(names(Nfreq2W) == "freq2W.X1")] <- "Freq"
colnames(Nfreq2W)[which(names(Nfreq2W) == "X2")] <- "term"
freq1W <- read.csv2(file="freq1W_th90.csv")
#freq1W <- read.csv2(file="freq1W.csv")
colnames(freq1W)[which(names(freq1W) == "X1")] <- "Freq"
setwd("~/R/LFG Working Directory/CAPSTONE/SHAPP")
shiny::runApp()
Nfreq3W <- read.csv2(file="Nfreq3W.csv")
colnames(Nfreq3W)[which(names(Nfreq3W) == "freq3W.X1")] <- "Freq"
colnames(Nfreq3W)[which(names(Nfreq3W) == "X3")] <- "term"
Nfreq2W <- read.csv2(file="Nfreq2W.csv")
colnames(Nfreq2W)[which(names(Nfreq2W) == "freq2W.X1")] <- "Freq"
colnames(Nfreq2W)[which(names(Nfreq2W) == "X2")] <- "term"
freq1W <- read.csv2(file="freq1W_th90.csv")
#freq1W <- read.csv2(file="freq1W.csv")
colnames(freq1W)[which(names(freq1W) == "X1")] <- "Freq"
View(Nfreq3W)
nextw <- function(word_seq, res){
ws_vec <- strsplit(as.character(word_seq), split=" ")
l <- length(ws_vec[[1]]) # Number of words in the sequence
if(substr(word_seq,(nchar(word_seq)+1)-1,nchar(word_seq)) == " ") l <- l+1
print(l)
if (l>=3){
# Search in trigram table based upon words -3, -2 and -1
ind <- which(Nfreq3W$X1==word(word_seq, -3))
df_tri <- Nfreq3W[ind,]
ind <- which(df_tri$X2==word(word_seq, -2))
df_tri <- df_tri[ind,]
ind <- grep(paste("^", word(word_seq, -1), sep=""), df_tri$term)
res <- df_tri[ind,]
if (length(res$X3)==0) l <- 2
}
if (l==2){
# Search in bigram table based upon words -2 and -1
#ind <- grep(paste("^", word(word_seq, -2), "$", sep=""), Nfreq2W$X1)
print(word(word_seq, -2))
ind <- which(Nfreq2W$X1==word(word_seq, -2))
df_bi <- Nfreq2W[ind,]
ind <- grep(paste("^", word(word_seq, -1), sep=""), df_bi$term)
res <- df_bi[ind,]
if (length(res$X2)==0) l <- 1
}
if (l==1) {
# Search in unigram table based upon word being typed
ind <- grep(paste("^", word(word_seq, -1), sep=""), freq1W$term)
res <- freq1W[ind,]
}
res <- head(res,3)
#res <- as.vector(res[,ncol(res)], mode="character")
print(l)
print(res)
#return(res)
#rownames(res) <- res[,ncol(res)]
return(res)
}
nextw("i ",)
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
library(devtools)
library(slidify)
author("SLIDIFY")
getwd()
setwd("~/R/LFG Working Directory/CAPSTONE/SLIDIFY")
slidify("index.Rmd")
library(slidify)
slidify("index.Rmd")
slidify("index.Rmd")
